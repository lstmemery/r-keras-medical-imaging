---
title: "Hello World for Medical Images"
author: "Matthew Emery"
date: "January 20, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(keras)
library(rprojroot)
library(fs)
```

### Installation Guide
Install R 3.5:
 - Windows: https://cran.r-project.org/bin/windows/base/
 - Mac: https://cran.r-project.org/bin/macosx/
 - Ubuntu: https://cran.r-project.org/bin/linux/ubuntu/README.html
Install RStudio Desktop (Free): https://www.rstudio.com/products/rstudio/download/#download
Download and extract the code to this talk: https://github.com/lstmemery/r-keras-medical-imaging/archive/master.zip
Click on the .Rproj file
`packrat::restore()` (This will take a while, please do it before the talk)
`keras::install_keras()`

### Who Am I?

- Alumni of the UBC Master of Data Science Program
- Previously: Software Developer at STEMCELL Technologies
- You can reach me at me@matthewemery.ca

### Acknowledgements
TODO: Acknowledge Fast.ai and the dataset creators

### Game Plan
TODO: Probably a lot more
 1. Defining some terms
 2. Example

### What is Machine Learning?

- Most of the time, programmers have input data and they apply a known function to get an unknown output
- Machine learning is when you have input data and known outputs and the program "learns" the rules to transform input to output TODO: Improve
- Examples: Linear Regression, Random Forest, Neural Networks

Aside: I'm talking only about supervised learning here

TODO: Image here?

### Why Should I Care About Neural Nets?

- Training is more "end-to-end" (i.e. they can learn features about data)
- They are much better at high visibility problems (Speech recognition, machine translation, computer vision)
- Tech companies open source really big neural networks that you can adapt for your problems

### What are Neural Nets? (Please, No Math)

- They are NOT electronic brains
- They are large arrays of numbers that get multiplied by one another
- In between those multiplications, we apply a special function to make sure the ordering of the arrays matter TODO: Clean I do better?
- We slightly adjust these numbers using a fancy version of first year calculus


### A Few Terms
- Loss Function: The function that the neural network is trying to minimize
- Batch Size: The number of images we feed into the neural net at a time
- Epochs: The number of times we see the whole training set

TODO: Get one of those dumb brain/machine images

TODO: Explain what this Dataset is


```{r}
# Constants
img_height <- 299L
img_width <- 299L
train_samples <- 65L
validation_samples <- 10L
epochs <- 20L
batch_size <- 5L
```

### How do we know if this will work?
- Separate your files into three groups, training, validation and testing
- Training images are the images that the neural net will use to update it's weights
- After we go through all the images once, check against the validation set
- If the loss for the training set goes down but the loss for the validation set goes up, the network may have overfit
- The test set is only used after the model is completely trained
TODO: Study, Midterm, Final analogy

```{r}
# Finding Training 
root <- find_rstudio_root_file()
train_data_directory <- path(root, "data", "TRAIN")
validation_data_directory <- path(root, "data", "VAL")
```

```{r}
knitr::include_graphics(path(train_data_directory, "openI_abd_xray", "openI_1.png"))
```

```{r}
knitr::include_graphics(path(train_data_directory, "openI_CXR", "3_IM-1384-1001.png"))
```

### Preparing images

- Notice that these images are different sizes
- Also the first image is slightly off-center
- We want the network to be robust, so we will randomly change each image slightly

TODO: Explain training generation

```{r}
train_data_generator <- image_data_generator(
  rescale = 1/255,
  shear_range = 0.2,
  zoom_range = 0.2,
  rotation_range = 20,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  horizontal_flip = TRUE
)
```

```{r}
validation_data_generator <- image_data_generator(
  rescale = 1/255
)
```

```{r}
train_generator <- train_data_generator$flow_from_directory(
  train_data_directory,
  target_size = c(img_height, img_width),
  batch_size = batch_size,
  class_mode = "binary"
)
```

```{r}
validation_generator <- validation_data_generator$flow_from_directory(
  validation_data_directory,
  target_size = c(img_height, img_width),
  batch_size = batch_size,
  class_mode = "binary"
)
```

```{r}
base_model <- application_inception_v3(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(img_width, img_height, 3L)
)
```


```{r}
for (layer in base_model$layers)
  layer$trainable <- TRUE
```


```{r}
model_top <- base_model$output %>% 
  layer_global_average_pooling_2d(trainable = TRUE) %>% 
  layer_dense(256L, 
              activation = "relu",
              trainable = TRUE) %>% 
  layer_dropout(0.5) %>% 
  layer_dense(1L,
              trainable = TRUE,
              activation = "sigmoid")
```

```{r}
full_model <- keras_model(
  inputs = base_model$input,
  outputs = model_top) %>% 
  keras::compile(
    optimizer = optimizer_adam(
      lr = 0.0001,
      epsilon = 1e-08
    ),
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )  
```

# TODO: Should probably freeze Inception weights

```{r}
history <- fit_generator(
  full_model,
  generator = train_generator,
  steps_per_epoch = as.integer(train_samples / batch_size),
  epochs = epochs,
  validation_data = validation_generator,
  validation_steps = as.integer(validation_samples / batch_size)
)
```

```{r}
summary(full_model)
```


## Bibliography
